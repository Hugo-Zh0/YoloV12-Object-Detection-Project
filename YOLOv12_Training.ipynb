{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Hugo-Zh0/YoloV12-Object-Detection-Project/blob/main/YOLOv12_Training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "980016c5",
      "metadata": {
        "id": "980016c5"
      },
      "source": [
        "\n",
        "# 🚀 YOLOv12 Object Detection Project\n",
        "\n",
        "[![Python](https://img.shields.io/badge/Python-3.10%2B-blue.svg)](https://www.python.org/downloads/)\n",
        "[![Anaconda](https://img.shields.io/badge/Anaconda-Navigator-green.svg)](https://www.anaconda.com/download)\n",
        "[![VS Code](https://img.shields.io/badge/Editor-VS%20Code-blue.svg)](https://code.visualstudio.com/)\n",
        "[![Ultralytics](https://img.shields.io/badge/YOLOv12-Ultralytics-yellow.svg)](https://github.com/ultralytics/ultralytics)\n",
        "[![License](https://img.shields.io/badge/License-MIT-lightgrey.svg)](LICENSE)\n",
        "\n",
        "---\n",
        "<br>\n",
        "\n",
        "## 📌 Overview\n",
        "A **collaborative group project** by Swinburne University students in partnership with **CSIRO**.  \n",
        "This repository contains the setup, configuration, and workflow for training and running **YOLOv12** object detection models.\n",
        "\n",
        "**👨‍💻 Team Members:** Harron, Feng, Bunmi, Huss, Hugo.\n",
        "\n",
        "---\n",
        "**Repo:** `Hugo-Zh0/YoloV12-Object-Detection-Project`  \n",
        "**What you’ll do:**\n",
        "1. Check runtime & GPU\n",
        "2. Install dependencies\n",
        "3. Clone your repo\n",
        "4. Set paths in repository\n",
        "5. Train\n",
        "6. Validate\n",
        "7. Predict\n",
        "8. Export\n",
        "10. Troubleshoot\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "24e32974",
      "metadata": {
        "id": "24e32974"
      },
      "source": [
        "## Step 1 — 🚀 Runtime & GPU check"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ada53746",
      "metadata": {
        "id": "ada53746",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "34f8f002-de27-4164-f716-989d6e3a6fd0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python: 3.12.11 (main, Jun  4 2025, 08:56:18) [GCC 11.4.0]\n",
            "Platform: Linux-6.1.123+-x86_64-with-glibc2.35\n",
            "PyTorch: 2.8.0+cu126\n",
            "CUDA available: True\n",
            "CUDA device: Tesla T4\n"
          ]
        }
      ],
      "source": [
        "\n",
        "#@title Check Python, CUDA, and PyTorch\n",
        "import sys, platform\n",
        "print(\"Python:\", sys.version)\n",
        "print(\"Platform:\", platform.platform())\n",
        "try:\n",
        "    import torch\n",
        "    print(\"PyTorch:\", torch.__version__)\n",
        "    print(\"CUDA available:\", torch.cuda.is_available())\n",
        "    if torch.cuda.is_available():\n",
        "        print(\"CUDA device:\", torch.cuda.get_device_name(0))\n",
        "except Exception as e:\n",
        "    print(\"PyTorch not installed yet (will install in next step).\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d0cbf86a",
      "metadata": {
        "id": "d0cbf86a"
      },
      "source": [
        "## Step 2 — ⬇️ Install dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7d0d0331",
      "metadata": {
        "id": "7d0d0331",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "ba714ef0-6da7-471f-df35-96d91bece26d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ultralytics\n",
            "  Downloading ultralytics-8.3.186-py3-none-any.whl.metadata (37 kB)\n",
            "Requirement already satisfied: numpy>=1.23.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (2.0.2)\n",
            "Requirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (3.10.0)\n",
            "Requirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (4.12.0.88)\n",
            "Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (11.3.0)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (6.0.2)\n",
            "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (2.32.4)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (1.16.1)\n",
            "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (2.8.0+cu126)\n",
            "Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (0.23.0+cu126)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from ultralytics) (5.9.5)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.12/dist-packages (from ultralytics) (9.0.0)\n",
            "Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (2.2.2)\n",
            "Collecting ultralytics-thop>=2.0.0 (from ultralytics)\n",
            "  Downloading ultralytics_thop-2.0.16-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.59.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (25.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.1.4->ultralytics) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.1.4->ultralytics) (2025.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (2025.8.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.19.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (4.14.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.4.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.8.0->ultralytics) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.8.0->ultralytics) (3.0.2)\n",
            "Downloading ultralytics-8.3.186-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m55.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ultralytics_thop-2.0.16-py3-none-any.whl (28 kB)\n",
            "Installing collected packages: ultralytics-thop, ultralytics\n",
            "Successfully installed ultralytics-8.3.186 ultralytics-thop-2.0.16\n",
            "Creating new Ultralytics Settings v0.0.6 file ✅ \n",
            "View Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\n",
            "Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n",
            "Ultralytics: 8.3.186\n",
            "Torch: 2.8.0+cu126 | CUDA: True\n",
            "OpenCV: 4.12.0\n"
          ]
        }
      ],
      "source": [
        "\n",
        "#@title Install Ultralytics & helpers\n",
        "!pip install ultralytics\n",
        "\n",
        "import torch, cv2, ultralytics\n",
        "print(\"Ultralytics:\", ultralytics.__version__)\n",
        "print(\"Torch:\", torch.__version__, \"| CUDA:\", torch.cuda.is_available())\n",
        "print(\"OpenCV:\", cv2.__version__)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "66a3d181",
      "metadata": {
        "id": "66a3d181"
      },
      "source": [
        "## Step 3 —  🤖 Clone your repository"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bc02da07",
      "metadata": {
        "id": "bc02da07",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd02947c-cb92-4252-c3e8-d392cfbb6416"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloned into: /content/YoloV12-Object-Detection-Project\n"
          ]
        }
      ],
      "source": [
        "\n",
        "#@title Clone repo\n",
        "REPO_URL = \"https://github.com/Hugo-Zh0/YoloV12-Object-Detection-Project\"\n",
        "REPO_DIR = \"/content/YoloV12-Object-Detection-Project\"\n",
        "\n",
        "import shutil, os, pathlib\n",
        "if os.path.isdir(REPO_DIR):\n",
        "    shutil.rmtree(REPO_DIR)\n",
        "\n",
        "!git clone -q {REPO_URL} {REPO_DIR}\n",
        "print(\"Cloned into:\", REPO_DIR)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then Next: Create 2 Folder called **yaml** and **datasets** inside the project repository"
      ],
      "metadata": {
        "id": "XHIlM3roXAfi"
      },
      "id": "XHIlM3roXAfi"
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Unzip Dataset and save to dataset folder\n",
        "!unzip -q /content/koala.zip -d /content/YoloV12-Object-Detection-Project/datasets/koala"
      ],
      "metadata": {
        "id": "gIpVYqfaR0zt"
      },
      "id": "gIpVYqfaR0zt",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Move YAML File to YAML Folder"
      ],
      "metadata": {
        "id": "5bzCZPa1S_fj"
      },
      "id": "5bzCZPa1S_fj"
    },
    {
      "cell_type": "markdown",
      "id": "0c20f512",
      "metadata": {
        "id": "0c20f512"
      },
      "source": [
        "## Step 4 — 🎞️ Set model & data paths"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Change YAML File for correct location\n",
        "change to /content/YoloV12-Object-Detection-Project/datasets/koala"
      ],
      "metadata": {
        "id": "_hWZ-cXcTWxo"
      },
      "id": "_hWZ-cXcTWxo"
    },
    {
      "cell_type": "markdown",
      "id": "b513b46d",
      "metadata": {
        "id": "b513b46d"
      },
      "source": [
        "## Step 5 — 💥 Train (set your parameters)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "11cd1e01",
      "metadata": {
        "id": "11cd1e01",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8726b9b9-5f93-4857-9400-6dfb96c53289"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model path: /content/YoloV12-Object-Detection-Project/models/yolo12s.pt\n",
            "Data yaml : /content/YoloV12-Object-Detection-Project/yaml/data.yaml\n",
            "Using device: cuda\n",
            "Ultralytics 8.3.186 🚀 Python-3.12.11 torch-2.8.0+cu126 CUDA:0 (Tesla T4, 15095MiB)\n",
            "\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=False, augment=False, auto_augment=randaugment, batch=12, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=/content/YoloV12-Object-Detection-Project/yaml/data.yaml, degrees=0.0, deterministic=True, device=0, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=100, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=512, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=/content/YoloV12-Object-Detection-Project/models/yolo12s.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=train11, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=100, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=None, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=runs/detect/train11, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=0, workspace=None\n",
            "Overriding model.yaml nc=80 with nc=1\n",
            "\n",
            "                   from  n    params  module                                       arguments                     \n",
            "  0                  -1  1       928  ultralytics.nn.modules.conv.Conv             [3, 32, 3, 2]                 \n",
            "  1                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
            "  2                  -1  1     26080  ultralytics.nn.modules.block.C3k2            [64, 128, 1, False, 0.25]     \n",
            "  3                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
            "  4                  -1  1    103360  ultralytics.nn.modules.block.C3k2            [128, 256, 1, False, 0.25]    \n",
            "  5                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n",
            "  6                  -1  2    689408  ultralytics.nn.modules.block.A2C2f           [256, 256, 2, True, 4]        \n",
            "  7                  -1  1   1180672  ultralytics.nn.modules.conv.Conv             [256, 512, 3, 2]              \n",
            "  8                  -1  2   2689536  ultralytics.nn.modules.block.A2C2f           [512, 512, 2, True, 1]        \n",
            "  9                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 10             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 11                  -1  1    345856  ultralytics.nn.modules.block.A2C2f           [768, 256, 1, False, -1]      \n",
            " 12                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 13             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 14                  -1  1     95104  ultralytics.nn.modules.block.A2C2f           [512, 128, 1, False, -1]      \n",
            " 15                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
            " 16            [-1, 11]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 17                  -1  1    296704  ultralytics.nn.modules.block.A2C2f           [384, 256, 1, False, -1]      \n",
            " 18                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n",
            " 19             [-1, 8]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 20                  -1  1   1511424  ultralytics.nn.modules.block.C3k2            [768, 512, 1, True]           \n",
            " 21        [14, 17, 20]  1    819795  ultralytics.nn.modules.head.Detect           [1, [128, 256, 512]]          \n",
            "YOLOv12s summary: 272 layers, 9,253,523 parameters, 9,253,507 gradients, 21.5 GFLOPs\n",
            "\n",
            "Transferred 685/691 items from pretrained weights\n",
            "Freezing layer 'model.21.dfl.conv.weight'\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access ✅ (ping: 0.0±0.0 ms, read: 1346.2±1312.2 MB/s, size: 198.6 KB)\n",
            "\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/YoloV12-Object-Detection-Project/datasets/koala/train/labels.cache... 390 images, 11 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 390/390 711208.1it/s 0.0s\n",
            "WARNING ⚠️ Box and segment counts should be equal, but got len(segments) = 6, len(boxes) = 417. To resolve this only boxes will be used and all segments will be removed. To avoid this please supply either a detect or segment dataset, not a detect-segment mixed dataset.\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
            "\u001b[34m\u001b[1mval: \u001b[0mFast image access ✅ (ping: 0.0±0.0 ms, read: 1192.1±1211.4 MB/s, size: 667.5 KB)\n",
            "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /content/YoloV12-Object-Detection-Project/datasets/koala/valid/labels.cache... 112 images, 1 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 112/112 222214.8it/s 0.0s\n",
            "WARNING ⚠️ Box and segment counts should be equal, but got len(segments) = 1, len(boxes) = 122. To resolve this only boxes will be used and all segments will be removed. To avoid this please supply either a detect or segment dataset, not a detect-segment mixed dataset.\n",
            "Plotting labels to runs/detect/train11/labels.jpg... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.002, momentum=0.9) with parameter groups 113 weight(decay=0.0), 120 weight(decay=0.00046875), 119 bias(decay=0.0)\n",
            "Image sizes 512 train, 512 val\n",
            "Using 0 dataloader workers\n",
            "Logging results to \u001b[1mruns/detect/train11\u001b[0m\n",
            "Starting training for 100 epochs...\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      1/100      14.4G      1.176      1.935      1.484         15        512: 100% ━━━━━━━━━━━━ 33/33 1.3it/s 24.8s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 5/5 2.6it/s 1.9s\n",
            "                   all        112        122      0.374      0.516      0.391      0.151\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      2/100      13.5G      1.465      1.824      1.698         11        512: 100% ━━━━━━━━━━━━ 33/33 1.3it/s 25.1s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 5/5 2.7it/s 1.9s\n",
            "                   all        112        122          0          0          0          0\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      3/100      13.5G      1.624      1.816       1.82         18        512: 100% ━━━━━━━━━━━━ 33/33 1.4it/s 23.4s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 5/5 2.8it/s 1.8s\n",
            "                   all        112        122      0.023      0.418     0.0172    0.00585\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      4/100      13.5G      1.622      1.738      1.813         13        512: 100% ━━━━━━━━━━━━ 33/33 1.4it/s 23.4s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 5/5 2.8it/s 1.8s\n",
            "                   all        112        122     0.0659      0.721      0.182     0.0657\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      5/100      13.5G      1.606      1.669      1.802         15        512: 100% ━━━━━━━━━━━━ 33/33 1.4it/s 23.4s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 5/5 2.8it/s 1.8s\n",
            "                   all        112        122      0.135      0.779      0.453      0.206\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      6/100      13.5G      1.497      1.598      1.726         19        512: 100% ━━━━━━━━━━━━ 33/33 1.4it/s 23.1s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 5/5 2.3it/s 2.1s\n",
            "                   all        112        122      0.163      0.795      0.457      0.181\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      7/100      13.5G      1.505      1.548      1.749          6        512: 100% ━━━━━━━━━━━━ 33/33 1.4it/s 23.2s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 5/5 2.7it/s 1.9s\n",
            "                   all        112        122      0.531      0.582      0.503      0.241\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      8/100      13.5G      1.488       1.52      1.714         16        512: 100% ━━━━━━━━━━━━ 33/33 1.4it/s 23.6s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 5/5 2.0it/s 2.5s\n",
            "                   all        112        122       0.46      0.459      0.378      0.165\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      9/100      13.5G      1.423      1.487      1.656         16        512: 100% ━━━━━━━━━━━━ 33/33 1.4it/s 24.4s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 5/5 2.8it/s 1.8s\n",
            "                   all        112        122      0.673      0.525      0.613      0.329\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     10/100      13.5G      1.334      1.352      1.555         14        512: 100% ━━━━━━━━━━━━ 33/33 1.4it/s 23.2s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 5/5 2.9it/s 1.7s\n",
            "                   all        112        122      0.701      0.639      0.755      0.441\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     11/100      13.5G      1.352      1.351      1.617         19        512: 100% ━━━━━━━━━━━━ 33/33 1.4it/s 23.6s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 5/5 2.9it/s 1.7s\n",
            "                   all        112        122      0.654      0.558      0.612      0.332\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     12/100      13.5G      1.331      1.299      1.589         21        512: 100% ━━━━━━━━━━━━ 33/33 1.4it/s 23.3s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 5/5 2.6it/s 1.9s\n",
            "                   all        112        122      0.651      0.639      0.644      0.355\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     13/100      13.5G      1.371      1.378        1.6         30        512:  70% ━━━━━━━━──── 23/33 1.5it/s 17.5s"
          ]
        }
      ],
      "source": [
        "from ultralytics import YOLO\n",
        "import torch, os, sys, requests\n",
        "\n",
        "# --- Direct paths (edit to your repo mount point in Colab) ---\n",
        "MODELS_DIR  = \"/content/YoloV12-Object-Detection-Project/models\"\n",
        "MODEL_PATH  = os.path.join(MODELS_DIR, \"yolo12s.pt\")\n",
        "DATA_YAML   = \"/content/YoloV12-Object-Detection-Project/yaml/data.yaml\"\n",
        "\n",
        "print(\"Model path:\", MODEL_PATH)\n",
        "print(\"Data yaml :\", DATA_YAML)\n",
        "\n",
        "# Device selection\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "# Load your local weights explicitly\n",
        "model = YOLO(MODEL_PATH)\n",
        "\n",
        "# Train without fetching any other checkpoints\n",
        "results = model.train(\n",
        "    data=DATA_YAML,\n",
        "    epochs=100,\n",
        "    imgsz=512,\n",
        "    batch=12,\n",
        "    workers=0,\n",
        "    device=device,\n",
        "    amp=False\n",
        ")\n",
        "\n",
        "print(\"Save dir:\", getattr(results, \"save_dir\", \"/content/YoloV12-Object-Detection-Project/runs/train\"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "68932001",
      "metadata": {
        "id": "68932001"
      },
      "source": [
        "## Step 6 — ✅ Validate best model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4f59c93e",
      "metadata": {
        "id": "4f59c93e"
      },
      "outputs": [],
      "source": [
        "\n",
        "#@title Validate best.pt\n",
        "from pathlib import Path\n",
        "from ultralytics import YOLO\n",
        "\n",
        "RUNS_DIR = Path(\"/content/runs/train\")\n",
        "assert RUNS_DIR.exists(), \"No training runs found. Train first.\"\n",
        "\n",
        "latest = sorted(RUNS_DIR.glob(\"**/\"), key=lambda p: p.stat().st_mtime)[-1]\n",
        "best = latest / \"weights\" / \"best.pt\"\n",
        "print(\"Using:\", best)\n",
        "assert best.exists(), f\"Missing best.pt in {latest}/weights\"\n",
        "\n",
        "model = YOLO(str(best))\n",
        "metrics = model.val(data=str(DATA_YAML), imgsz=512, device=\"cuda\" if __import__(\"torch\").cuda.is_available() else \"cpu\")\n",
        "print(metrics)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b9401666",
      "metadata": {
        "id": "b9401666"
      },
      "source": [
        "## Step 7 — 🔮 Predict (images)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9ea59f46",
      "metadata": {
        "id": "9ea59f46"
      },
      "outputs": [],
      "source": [
        "\n",
        "#@title Upload images and predict\n",
        "from google.colab import files\n",
        "from ultralytics import YOLO\n",
        "from pathlib import Path\n",
        "import torch\n",
        "\n",
        "uploaded = files.upload()\n",
        "img_paths = list(uploaded.keys())\n",
        "print(\"Uploaded:\", img_paths)\n",
        "\n",
        "# choose model: latest best or repo weights\n",
        "RUNS_DIR = Path(\"/content/runs/train\")\n",
        "best_path = None\n",
        "if RUNS_DIR.exists():\n",
        "    cands = sorted(RUNS_DIR.glob(\"**/weights/best.pt\"), key=lambda p: p.stat().st_mtime)\n",
        "    if cands:\n",
        "        best_path = str(cands[-1])\n",
        "model_path = best_path if best_path else str(Path(\"/content/YoloV12-Object-Detection-Project\") / \"models\" / \"yolo12s.pt\")\n",
        "\n",
        "model = YOLO(model_path)\n",
        "pred = model.predict(source=img_paths, imgsz=512, device=\"cuda\" if torch.cuda.is_available() else \"cpu\", save=True)\n",
        "print(\"Saved to:\", pred[0].save_dir if isinstance(pred, list) and pred else \"Check /content/runs/predict\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1d323b1e",
      "metadata": {
        "id": "1d323b1e"
      },
      "source": [
        "## Step 8 — 📺 Predict (video)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "57c872d2",
      "metadata": {
        "id": "57c872d2"
      },
      "outputs": [],
      "source": [
        "\n",
        "#@title Predict on a video (URL or upload)\n",
        "video_url = \"\"  #@param {type:\"string\"}\n",
        "\n",
        "import urllib.request, os\n",
        "from google.colab import files\n",
        "from ultralytics import YOLO\n",
        "from pathlib import Path\n",
        "import torch\n",
        "\n",
        "src = None\n",
        "if video_url.strip():\n",
        "    src = \"/content/input_video.mp4\"\n",
        "    try:\n",
        "        urllib.request.urlretrieve(video_url, src)\n",
        "        print(\"Downloaded:\", src)\n",
        "    except Exception as e:\n",
        "        print(\"Failed to download:\", e)\n",
        "        src = None\n",
        "\n",
        "if not src:\n",
        "    up = files.upload()\n",
        "    if up:\n",
        "        src = list(up.keys())[0]\n",
        "        print(\"Uploaded:\", src)\n",
        "\n",
        "assert src, \"No video provided.\"\n",
        "\n",
        "RUNS_DIR = Path(\"/content/runs/train\")\n",
        "best_path = None\n",
        "if RUNS_DIR.exists():\n",
        "    cands = sorted(RUNS_DIR.glob(\"**/weights/best.pt\"), key=lambda p: p.stat().st_mtime)\n",
        "    if cands:\n",
        "        best_path = str(cands[-1])\n",
        "model_path = best_path if best_path else str(Path(\"/content/YoloV12-Object-Detection-Project\") / \"models\" / \"yolov12s.pt\")\n",
        "\n",
        "model = YOLO(model_path)\n",
        "pred = model.predict(source=src, imgsz=512, device=\"cuda\" if torch.cuda.is_available() else \"cpu\", save=True)\n",
        "print(\"Video predictions saved to:\", pred[0].save_dir if isinstance(pred, list) and pred else \"Check /content/runs/predict\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c7247239",
      "metadata": {
        "id": "c7247239"
      },
      "source": [
        "## Step 9 — ♟️ Exporting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f67cee5d",
      "metadata": {
        "id": "f67cee5d"
      },
      "outputs": [],
      "source": [
        "## Nothing here yet"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "00b4bd4d",
      "metadata": {
        "id": "00b4bd4d"
      },
      "source": [
        "## Troubleshooting"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "35f7a69d",
      "metadata": {
        "id": "35f7a69d"
      },
      "source": [
        "\n",
        "- **Weights YAML missing:** ensure `models/yolov12s.pt` and `yaml/data.yaml` exist in the repo or update paths.\n",
        "- **No GPU:** Colab may not offer a GPU; the notebook will use CPU (much slower).\n",
        "- **Val fails:** Train first; then rerun the validate cell.\n",
        "- **Poor metrics:** Add more data, correct labels, tune `imgsz`/`batch`/`epochs`.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}